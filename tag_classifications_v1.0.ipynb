{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "visible-train",
   "metadata": {},
   "source": [
    "# TAG CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-trinity",
   "metadata": {},
   "source": [
    "In the first version of tag classifications we will try to predict the tag labels for issues based only on the description. More precise, we will use our word embeddings which have already been created to compute the arithmetic representation of the description and then based on that we will try to predict the type of issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-shoot",
   "metadata": {},
   "source": [
    "Our first try is based on logistic regression. Logistic regression used for binary classification but using the method one vs rest we can train one logistic regression model for each label.  Maybe one better version will be using the multinomial logistic regression\n",
    "\n",
    "Moreover, for the arithmetic representation of first we will use the average of the word embeddings and maybe in later stage we will try to improve the formula using a weighted average based on TF-IDF method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-jaguar",
   "metadata": {},
   "source": [
    "## PRE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-interstate",
   "metadata": {},
   "source": [
    "First, load the word embeddings, the vocabulary and for every issue the corresponding tags and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "mounted-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "upset-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_issues(dir_path,tag_labels,descriptions):\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "\n",
    "            data = json.load(json_file)\n",
    "            for issue in data:\n",
    "                \n",
    "                name = issue['name']\n",
    "                tags = issue['tags']\n",
    "                for i in range(len(tags)):\n",
    "                    tags[i] = tags[i].strip()\n",
    "                description = issue['description']\n",
    "                \n",
    "                if tags != []:\n",
    "                    tag_labels.append(tags)\n",
    "                    descriptions.append(description)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "outside-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_desc(descriptions,word2id):\n",
    "    \n",
    "    arithmetic_descriptions = list()\n",
    "    \n",
    "    #define stop words\n",
    "    all_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    for desc in descriptions:\n",
    "        \n",
    "        #join all lines into one sentence\n",
    "        sentence     = ' '.join(desc)\n",
    "        \n",
    "        #translate punctuation\n",
    "        new_sentence = sentence.translate(translator)\n",
    "        #split the sentense in words\n",
    "        words = new_sentence.split()\n",
    "        words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "        \n",
    "        arithmetic_desc = list()\n",
    "        \n",
    "        for w in words_sw:\n",
    "            arithmetic_desc.append(word2id.get(w,-2))\n",
    "            \n",
    "        arithmetic_descriptions.append(arithmetic_desc)\n",
    "        \n",
    "    #clean the list descriptions because its useless\n",
    "    descriptions = []\n",
    "    \n",
    "    return arithmetic_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "premier-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For beggining we will compute description embeddings just by averaging all the embeddings for every word in \n",
    "# the description\n",
    "\n",
    "def compute_descriptions_embeddings(arithmetic_descriptions, word_embeddings_matrix):\n",
    "    \n",
    "    embedding_dim    = np.shape(word_embeddings_matrix)[1]\n",
    "    num_descriptions = len(arithmetic_descriptions)\n",
    "    \n",
    "    descriptions_embeddings = np.zeros((num_descriptions,embedding_dim))\n",
    "    \n",
    "    for counter,desc in enumerate(arithmetic_descriptions):\n",
    "        total_words    = 0 \n",
    "        \n",
    "        for word in desc:\n",
    "            if word != -2:\n",
    "                total_words   += 1\n",
    "                descriptions_embeddings[counter] = descriptions_embeddings[counter] + word_embedding_matrix[word]\n",
    "        \n",
    "        if total_words != 0:\n",
    "            descriptions_embeddings[counter] /= total_words\n",
    "       \n",
    "    return descriptions_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "warming-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load word embeddings\n",
    "word_embedding_matrix = np.loadtxt('word_embeddings.txt', dtype=np.float64)\n",
    "\n",
    "#load vocabulary\n",
    "word2id = dict()\n",
    "id2word = dict()\n",
    "\n",
    "file = open(\"vocabulary.txt\",\"r\")\n",
    "lines = file.readlines()\n",
    "for line in lines:\n",
    "    temp = str(line)\n",
    "    values = temp.split(',')\n",
    "    word2id[values[0]] = int(values[1].replace(\"\\n\",\"\"))\n",
    "    id2word[int(values[1].replace(\"\\n\",\"\"))] = values[0]\n",
    "    \n",
    "#load tags and descriptions\n",
    "dir_path     = '/home/kostas/Documents/thesis/data_1'\n",
    "tag_labels   = list()\n",
    "descriptions = list()\n",
    "\n",
    "load_issues(dir_path,tag_labels,descriptions)\n",
    "\n",
    "# some prints for debugging purposes\n",
    "#print(len(tag_labels))\n",
    "#print(len(descriptions))\n",
    "#for i in range(0,len(tag_labels)):\n",
    "#    print(\"tags:\",tag_labels[i])\n",
    "#    print(\"description:\",descriptions[i])\n",
    "#    print(\"\\n\")\n",
    "\n",
    "arithmetic_descriptions = transform_desc(descriptions,word2id)\n",
    "descriptions_embeddings = compute_descriptions_embeddings(arithmetic_descriptions,word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "congressional-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['Bug','Google Play or Beta feedback','Feedback required','Feature Request','Unverified','Frontend Design']\n",
    "no_tags = 6\n",
    "np_tags = np.zeros((len(arithmetic_descriptions),no_tags))\n",
    "\n",
    "for counter in range(len(tag_labels)):\n",
    "    for counter_2,value in enumerate(tags):\n",
    "        if value in tag_labels[counter]:\n",
    "            np_tags[counter][counter_2] = 1\n",
    "\n",
    "df_tags = pd.DataFrame(np_tags, columns = tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "convinced-democrat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bug classifier,mean accuracy 0.722\n",
      "Google Play or Beta feedback classifier,mean accuracy 0.961\n",
      "Feedback required classifier,mean accuracy 0.909\n",
      "Feature Request classifier,mean accuracy 0.814\n",
      "Unverified classifier,mean accuracy 0.883\n",
      "Frontend Design classifier,mean accuracy 0.880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression classification model for the label bug\n",
    "bug_series = df_tags[\"Bug\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Bug classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "# train a logistic regression classification model for the label Google Play or Beta feedback\n",
    "bug_series = df_tags[\"Google Play or Beta feedback\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Google Play or Beta feedback classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "# train a logistic regression classification model for the label Feedback required\n",
    "bug_series = df_tags[\"Feedback required\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Feedback required classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "# train a logistic regression classification model for the label Feature Request\n",
    "bug_series = df_tags[\"Feature Request\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Feature Request classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "# train a logistic regression classification model for the label Unverified\n",
    "bug_series = df_tags[\"Unverified\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Unverified classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "# train a logistic regression classification model for the label Frontend Design\n",
    "bug_series = df_tags[\"Frontend Design\"]\n",
    "model      = LogisticRegression(solver='lbfgs')\n",
    "skf        = StratifiedKFold(n_splits = 10)\n",
    "scores     = cross_val_score(model,descriptions_embeddings,bug_series,scoring='accuracy',cv = skf)\n",
    "print(\"Frontend Design classifier,mean accuracy %.3f\"%np.mean(scores))\n",
    "skf.get_n_splits(descriptions_embeddings,bug_series)\n",
    "\n",
    "#for train_index, test_index in skf.split(descriptions_embeddings,bug_series):\n",
    "#    X_train, X_test = descriptions_embeddings[train_index],descriptions_embeddings[test_index],\n",
    "#    y_train, y_test = bug_series[train_index], bug_series[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "posted-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3843\n",
      "<class 'numpy.ndarray'>\n",
      "3842\n",
      "3842\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id))\n",
    "print(type(word_embedding_matrix))\n",
    "print(len(word_embedding_matrix))\n",
    "num_rows,num_col = word_embedding_matrix.shape\n",
    "print(num_rows)\n",
    "print(num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "mexican-suggestion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_dict = dict()\n",
    "for labels in tag_labels:\n",
    "    for label in labels:\n",
    "        if label not in tag_dict:\n",
    "            tag_dict[label]  = 1\n",
    "        else:\n",
    "            tag_dict[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "assigned-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Bug  , 2481\n",
      "                 Map: Mapsforge  , 73\n",
      "                 Unverified  , 591\n",
      "                 Feature Request  , 942\n",
      "                 Feedback required  , 461\n",
      "                 CI server / Build tools  , 135\n",
      "                 Google Play or Beta feedback  , 215\n",
      "                 Connector OC  , 127\n",
      "                 Connector GK  , 46\n",
      "                 Prio - High  , 447\n",
      "                 Duplicate  , 328\n",
      "                 Frontend Design  , 605\n",
      "                 Won't Fix  , 184\n",
      "                 Prio - Low  , 490\n",
      "                 Prio - Medium  , 199\n",
      "                 Assigned-non-collab  , 86\n",
      "                 Refactoring  , 172\n",
      "                 Send2cgeo  , 38\n",
      "                 Website  , 80\n",
      "                 Connector GC  , 20\n",
      "                 Regression  , 168\n",
      "                 Performance  , 10\n",
      "                 Translation  , 77\n",
      "                 Hacktoberfest  , 11\n",
      "                 Map: GMapsV2  , 34\n",
      "                 Add-on: Contacts  , 8\n",
      "                 Field test  , 33\n",
      "                 Connector EC  , 17\n",
      "                 Live Map  , 141\n",
      "                 Regression SDK30/SAF  , 38\n",
      "                 Regression SDK26  , 39\n",
      "                 Good first issue  , 4\n",
      "                 Connector SU  , 13\n",
      "                 Wiki  , 4\n",
      "                 WP Calc  , 28\n",
      "                 Do not merge / WIP  , 2\n",
      "                 Legacy  , 2\n"
     ]
    }
   ],
   "source": [
    "for key in tag_dict:\n",
    "    print(key,\",\",tag_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# issues with no tags i will not use them, because i dont know where they belong\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
