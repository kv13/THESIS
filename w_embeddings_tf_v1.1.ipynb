{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-simpson",
   "metadata": {},
   "source": [
    "# WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-patient",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-meeting",
   "metadata": {},
   "source": [
    "In the pre processing the goal is to split dataset on training, validation and testing dataset. In order to achieve a better split first we find the average number of words each description has. Then based on that we compute how many descriptions are necessary for each set using the rule of thump, which is 80%-20% split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-ownership",
   "metadata": {},
   "source": [
    "After the splitting based only on training set we create the vocabulary remove rare words \n",
    "and define model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reflected-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labriares\n",
    "\n",
    "import os \n",
    "import json \n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import collections\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "finite-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is only for debugging purposes.\n",
    "# it search a string in the descriptions and print which \n",
    "# description contains it \n",
    "def search_string(descriptions,word_to_search):\n",
    "    for counter,value in enumerate(descriptions):\n",
    "        if word_to_search in value:\n",
    "            print(\"found the word on issue\",counter+1)\n",
    "            print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "valuable-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pre_processing function loads all the descriptions in a list. Split every description in \n",
    "# words then cleans the data from stop words, punctuation and lowers all letters.\n",
    "\n",
    "def pre_processing(dir_path,descriptions,all_stopwords):\n",
    "    \n",
    "    total_words    = 0\n",
    "    counter        = 0\n",
    "    counter_issues = 0\n",
    "    \n",
    "    for fname in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path,fname)) as json_file:\n",
    "            counter += 1\n",
    "            print(counter,\") reading file\",fname)\n",
    "            \n",
    "            #load data in json format\n",
    "            data = json.load(json_file)\n",
    "            for p in data:\n",
    "                \n",
    "                ##############################\n",
    "                issue_name     = p['name']\n",
    "                counter_issues += 1\n",
    "                print(\"  \",counter_issues,\")\",issue_name)\n",
    "                ##############################\n",
    "                \n",
    "                issue_desc     = p['description'] \n",
    "                clean_desc     = clean_data(issue_desc,all_stopwords)\n",
    "                total_words    = total_words + len(clean_desc)\n",
    "                descriptions.append(clean_desc)\n",
    "            \n",
    "    return len(descriptions),total_words/len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "southeast-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(description,all_stopwords):\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #join all lines into one sentence\n",
    "    sentence     = ' '.join(description)\n",
    "    \n",
    "    #translate punctuation\n",
    "    new_sentence = sentence.translate(translator)\n",
    "    \n",
    "    #split the sentense in words\n",
    "    words = new_sentence.split()\n",
    "    words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "    \n",
    "    return words_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "legislative-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits the dataset into training validation and testing dataset.\n",
    "# It randomly selects test_size descriptions for testing \n",
    "# and valid_size descriptions for validation. \n",
    "# Also keeps the initial indexes for debugging purposes.\n",
    "\n",
    "def split_dataset(descriptions,train_set,valid_set,valid_size,\n",
    "                  test_set,test_size,words_per_desc):\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    valid_index   = []\n",
    "    test_index    = []\n",
    "    \n",
    "    #random select valid size descriptions from the dataset\n",
    "    for i in range(valid_size):\n",
    "        flag = True\n",
    "        while flag:\n",
    "            temp = randint(0,num_issues-1)\n",
    "            if temp not in valid_index and len(descriptions[temp])>= 2*words_per_desc:\n",
    "                valid_index.append(temp)\n",
    "                flag = False\n",
    "    \n",
    "    #random select testing size descriptions from the dataset\n",
    "    for i in range(test_size):\n",
    "        flag = True\n",
    "        while flag:\n",
    "            temp = randint(0,num_issues-1)\n",
    "            if temp not in valid_index and temp not in test_index and len(descriptions[temp])>= 2*words_per_desc:\n",
    "                test_index.append(temp)\n",
    "                flag = False\n",
    "    \n",
    "    for i in valid_index:\n",
    "        valid_set.append(descriptions[i])\n",
    "    \n",
    "    for i in test_index:\n",
    "        test_set.append(descriptions[i])\n",
    "        \n",
    "    for i in range(len(descriptions)):\n",
    "        if i not in valid_index and i not in test_index:\n",
    "            train_set.append(descriptions[i])\n",
    "    \n",
    "    descriptions = []\n",
    "    \n",
    "    #debugging prints\n",
    "    #print(\"the issues used for validation are\")\n",
    "    #for i in valid_index:\n",
    "    #    print(i)\n",
    "    #print(\"#############\")\n",
    "    #print(\"the issues used for testing are\")\n",
    "    #for i in test_index:\n",
    "    #    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "banned-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function as before but its more efficient \n",
    "# but doesnt keep the indexes of the description used for validation and testing\n",
    "# for debugging purposes the first one is used\n",
    "\n",
    "def split_dataset2(descriptions,train_set,valid_set,valid_size,\n",
    "                   test_set,test_size,words_per_desc):\n",
    "    \n",
    "    seed(datetime.now())\n",
    "    \n",
    "    for i in range(valid_size):\n",
    "        temp = randint(0,len(descriptions)-1)\n",
    "        if len(descriptions[temp])>= 2*words_per_desc:\n",
    "            valid_set.append(descriptions.pop(temp))\n",
    "        \n",
    "    for i in range(test_size):\n",
    "        temp = randint(0,len(descriptions)-1)\n",
    "        if len(descriptions[temp])>= 2*words_per_desc:\n",
    "            test_set.append(descriptions.pop(temp))\n",
    "        \n",
    "    train_set = descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary parameters\n",
    "dir_path        = '/home/kostas/Documents/thesis/data_1'\n",
    "descriptions    = []\n",
    "\n",
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#pre processining stage starts\n",
    "num_issues,mean_words    = pre_processing(dir_path,descriptions,all_stopwords) \n",
    "\n",
    "# for validation and testing we will random choose 1/10 words from every desc.\n",
    "words_per_desc = int(mean_words // 10)\n",
    "\n",
    "# validation set has length 100 words\n",
    "valid_words = 100\n",
    "valid_size  = int(valid_words // words_per_desc)\n",
    "\n",
    "# testing set has length 200 words\n",
    "test_words = 200\n",
    "test_size  = int(test_words // words_per_desc)\n",
    "\n",
    "# split dataset\n",
    "\n",
    "train_set = []\n",
    "valid_set = []\n",
    "test_set  = []\n",
    "\n",
    "#search a string in descriptions only for debugging purposes.\n",
    "#search_string(descriptions,\"cachedetailactivity\")\n",
    "\n",
    "split_dataset(descriptions,train_set,valid_set,valid_size,test_set,test_size,words_per_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stuck-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sets for debugging\n",
    "# print(train_set)\n",
    "# print(valid_set)\n",
    "# print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "authorized-animal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total issues 5993\n",
      "average number of words per description 41.79726347405306\n",
      "size of validation set 25\n",
      "size of test set 50\n"
     ]
    }
   ],
   "source": [
    "# print messages #\n",
    "print(\"total issues\",num_issues)\n",
    "print(\"average number of words per description\",mean_words)\n",
    "print(\"size of validation set\",valid_size)\n",
    "print(\"size of test set\",test_size)\n",
    "#print(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-attempt",
   "metadata": {},
   "source": [
    "## Compute Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-allergy",
   "metadata": {},
   "source": [
    "After the pre processing step, now its time to create the vocabulary and the skip-grams pairs and train the model. Every model parameter is a list, so we compute embeddings for every combination. Testing every combination on validation set and keep the model with the best resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capable-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim_list = [25,50,100,150,200]\n",
    "learning_rate_list = [0.01,0.1,1]\n",
    "skip_window_list   = [2,4,6,8]\n",
    "num_sampled_list   = [32,64,100,128]\n",
    "num_epochs_list    = [10,20,50,100]\n",
    "\n",
    "unk_word      = \"UNK\"\n",
    "batch_size    = 100\n",
    "min_occurance = 5\n",
    "num_skips     = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "discrete-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(word_dict):\n",
    "    file = open(\"vocabulary.txt\",\"w\")\n",
    "    for key in word_dict:\n",
    "        file.write(\"%s, %s \\n\"%(key,str(word_dict[key])))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "novel-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vocabulary \n",
    "#remove rare words from vocabulary which occurrs less time than min_occurance\n",
    "\n",
    "#word2id     :dictionary which contains the vocabulary and it's int id\n",
    "#id2word     :dictionary which assigns its id to corresponding word\n",
    "\n",
    "temp_sentences = []\n",
    "for i in train_set:\n",
    "    for j in i:\n",
    "        temp_sentences.append(j)\n",
    "    \n",
    "count  = []\n",
    "count.extend(collections.Counter(temp_sentences).most_common())\n",
    "\n",
    "# list temp_sentences now is useless\n",
    "temp_sentences = []\n",
    "\n",
    "for i in range(len(count)-1,-1,-1):\n",
    "    if count[i][1]<min_occurance:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "            \n",
    "#compute the vocabulary size\n",
    "vocabulary_size = len(count)\n",
    "\n",
    "#assign an id to each word\n",
    "word2id = dict()\n",
    "word2id[unk_word] = -2\n",
    "\n",
    "for i, (word,_) in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "#list count now is useless\n",
    "count = []\n",
    "\n",
    "#express train, valid and test set using id\n",
    "train_set_id = list()\n",
    "valid_set_id = list()\n",
    "test_set_id  = list()\n",
    "\n",
    "for desc in train_set:   \n",
    "    temp_desc = []\n",
    "    for j in desc:\n",
    "        temp_desc.append(word2id.get(j,-2))\n",
    "    \n",
    "    #make sure that there are more than num_skips words in order to take account the description\n",
    "    counter = 0\n",
    "    for i in temp_desc:\n",
    "        if i != -2:\n",
    "            counter += 1\n",
    "        \n",
    "    if counter > num_skips :\n",
    "        train_set_id.append(temp_desc)\n",
    "    \n",
    "#list train_set now is useless\n",
    "train_set = []\n",
    "\n",
    "for desc in valid_set:\n",
    "    temp_desc = []\n",
    "    for j in desc:\n",
    "        temp_desc.append(word2id.get(j,-2))\n",
    "    valid_set_id.append(temp_desc)\n",
    "    \n",
    "#list valid_set now is useless\n",
    "valid_set = []\n",
    "\n",
    "for desc in test_set:\n",
    "    temp_desc = []\n",
    "    for j in desc:\n",
    "        temp_desc.append(word2id.get(j,-2))\n",
    "    test_set_id.append(temp_desc)\n",
    "\n",
    "#list test_set now is useless\n",
    "test_set = []\n",
    "\n",
    "#save the vocabulary in file \n",
    "save_vocabulary(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "raising-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some prints for debugging purposes\n",
    "#print(len(temp_sentences))\n",
    "\n",
    "#print(word2id)\n",
    "\n",
    "#for i in range(len(count)-1,-1,-1):\n",
    "#    print(count[i])\n",
    "\n",
    "#for i in range(len(count)-1,-1,-1):\n",
    "#    if count[i][0] == '20cachetag':\n",
    "#        print(count[i])\n",
    "\n",
    "#total_words = 0\n",
    "#for i in train_set_id:\n",
    "#    total_words += len(i)\n",
    "#print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adjusted-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for every description in the description_set choose randomly words_per_desc\n",
    "# words and create num_skips inside the skip_window.\n",
    "# The only restriction is the chosen word to be in the dictionary\n",
    "\n",
    "def testing_skip_grams(list_grams,skip_window,num_skips,\n",
    "                       descriptions_list,words_per_desc):\n",
    "    \n",
    "    #an important constraint\n",
    "    assert num_skips<=skip_window\n",
    "    \n",
    "    span = 2*skip_window+1\n",
    "    \n",
    "    for desc in descriptions_list:\n",
    "        \n",
    "        #for debugging\n",
    "        #print(\"Description:\",desc)\n",
    "        \n",
    "        seed(datetime.now())\n",
    "        target_words = []\n",
    "        for i in range(words_per_desc):\n",
    "            flag = True\n",
    "            skip_gram_list = []\n",
    "            while flag: \n",
    "                temp = randint(0,len(desc)-1)\n",
    "                \n",
    "                if desc[temp] not in target_words and desc[temp] != -2:\n",
    "                    \n",
    "                    flag = False\n",
    "                    target_words.append(desc[temp])\n",
    "                    \n",
    "                    temp_list     = []\n",
    "                    context_words = []\n",
    "\n",
    "                    # create skip-grams for the target word.\n",
    "                    find_context_words(desc,temp,skip_window,span,temp_list)\n",
    "                    \n",
    "                    #take num_skips random samples\n",
    "                    context_words = [w for w in range(1,len(temp_list))]\n",
    "                    words_to_use  = random.sample(context_words,num_skips)\n",
    "                    \n",
    "                    skip_gram_list.append(temp_list[0])\n",
    "                    for temp_word in words_to_use:\n",
    "                        skip_gram_list.append(temp_list[temp_word])\n",
    "                        \n",
    "            \n",
    "            list_grams.append(skip_gram_list)\n",
    "            ## some print messages for debugging purposes.\n",
    "            #print(\"target word:\",target_words[i]) \n",
    "            #print(\"temp_list:\",temp_list)\n",
    "            #print(\"words to use:\",words_to_use)\n",
    "            #print(\"skip-grams:\",skip_gram_list)\n",
    "        \n",
    "        #print(\"choosen word:\",target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "female-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(description,word_index,skip_window,span,grams_list):\n",
    "    \n",
    "    # the target word in the first place\n",
    "    grams_list.append(description[word_index])\n",
    "    \n",
    "    # initialize two pointer\n",
    "    counter = 1\n",
    "    data_index = word_index-1\n",
    "    \n",
    "    while counter<span:\n",
    "        #look left from target word\n",
    "        if counter<=skip_window:\n",
    "            #if data_index<0 => out of bound no more words to take into account\n",
    "            if data_index<0:\n",
    "                data_index = word_index + 1\n",
    "                counter    = skip_window + 1\n",
    "            #if the word is not in the dictionary skip it\n",
    "            elif description[data_index] == -2:\n",
    "                #print(\"before:\",data_index)\n",
    "                #print(description[data_index],word2id.get(description[data_index],-2))\n",
    "                data_index -= 1 \n",
    "                #print(\"after:\",data_index)\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index -= 1\n",
    "                if counter >skip_window:\n",
    "                    data_index = word_index+1\n",
    "        #look right from target word\n",
    "        else:\n",
    "            if data_index>=len(description):\n",
    "                counter = span + 1\n",
    "            elif description[data_index] == -2:\n",
    "                #print(description[data_index],word2id.get(description[data_index],-2))\n",
    "                data_index += 1\n",
    "            else:\n",
    "                grams_list.append(description[data_index])\n",
    "                counter    += 1\n",
    "                data_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "viral-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grams_to_file(test_grams):\n",
    "    with open('test_set.txt','w') as f:\n",
    "        for item in test_grams:\n",
    "            for i in item:\n",
    "                f.write(\"%s, \"%i)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hidden-macro",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create validation and test pairs\n",
    "valid_grams = []\n",
    "test_grams  = []\n",
    "\n",
    "#create 2 pairs for every word in a window of size of 2.\n",
    "#create skip grams for test set\n",
    "testing_skip_grams(test_grams, 2, 2, test_set_id, words_per_desc)\n",
    "\n",
    "#save the test grams in a file in order not to overload memory.\n",
    "save_grams_to_file(test_grams)\n",
    "\n",
    "#test grams now are useless\n",
    "test_grams = []\n",
    "\n",
    "#create skip grams for valid set.\n",
    "testing_skip_grams(valid_grams,2,2,valid_set_id,words_per_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-nirvana",
   "metadata": {},
   "source": [
    "## Model Definition and Word Embedding Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-sodium",
   "metadata": {},
   "source": [
    "In this section, all functions are asocciated with the definition of the model the training process and last but not least the evaluation of the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "perfect-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dental-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size,num_skips,skip_window,train_set_id,word_pointer,desc_pointer,epoch):\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= skip_window\n",
    "    \n",
    "    # the batch stores target words\n",
    "    batch = np.ndarray(shape = (batch_size),dtype = np.int32)\n",
    "    # labels are the context words=>(skip-grams)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    span = 2*skip_window+1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "         \n",
    "        #avoid tags with -2 \n",
    "        while train_set_id[desc_pointer][word_pointer] <0:\n",
    "            word_pointer += 1\n",
    "            if word_pointer > len(train_set_id[desc_pointer])-1:\n",
    "                word_pointer  = 0\n",
    "                desc_pointer += 1\n",
    "                if desc_pointer > len(train_set_id)-1:\n",
    "                    desc_pointer =0\n",
    "                    epoch += 1\n",
    "                \n",
    "        find_context_words(train_set_id[desc_pointer],word_pointer,skip_window,span,buffer)\n",
    "        \n",
    "        #take num_skips random samples \n",
    "        context_words = [w for w in range(1,len(buffer))]\n",
    "        words_to_use  = random.sample(context_words,num_skips)\n",
    "        \n",
    "        # print(\"description\",desc_pointer, \"target word\",word_pointer,\"words_to_use:\",words_to_use,\"buffer:\",buffer)\n",
    "        \n",
    "        # update batch and labels\n",
    "        for j,random_word in enumerate(words_to_use):\n",
    "            batch[i*num_skips+j]    = buffer[0]\n",
    "            labels[i*num_skips+j,0] = buffer[random_word]\n",
    "            \n",
    "        buffer.clear()\n",
    "        \n",
    "        if word_pointer == len(train_set_id[desc_pointer])-1:\n",
    "            \n",
    "            word_pointer  = 0\n",
    "            desc_pointer += 1\n",
    "            if desc_pointer > len(train_set_id)-1:\n",
    "                desc_pointer = 0\n",
    "                epoch += 1\n",
    "        else:\n",
    "            word_pointer += 1\n",
    "            \n",
    "    return batch,labels,epoch,word_pointer,desc_pointer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "vocational-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_def(train_set_id,batch_size,embedding_dim,skip_window,\n",
    "              num_skips,num_sampled,learning_rate,vocabulary_size,total_epochs,testing_grams):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Input data\n",
    "    X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "    # Input label\n",
    "    Y_train = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "    \n",
    "    #ensure that the following ops & var are assigned to CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        #create the embedding variable wich contains the weights\n",
    "        embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "        \n",
    "        #create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "        X_embed   = tf.nn.embedding_lookup(embedding,X_train) \n",
    "        \n",
    "        #create variables for the loss function\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_dim],stddev=1.0))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "    loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases,\n",
    "                                              labels = Y_train,inputs = X_embed, \n",
    "                                              num_sampled = num_sampled,\n",
    "                                              num_classes = vocabulary_size ))\n",
    "        \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        \n",
    "    train_opt = optimizer.minimize(loss_func)\n",
    "        \n",
    "    #Define initializer for tensorflow variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #actual initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        epoch        = 0 \n",
    "        average_loss = 0\n",
    "        desc_pointer = 0\n",
    "        word_pointer = 0\n",
    "        step_counter = 0\n",
    "        av_losses_list = []\n",
    "        \n",
    "        while epoch <= total_epochs-1:\n",
    "            \n",
    "            step_counter += 1\n",
    "            \n",
    "            #take new batch of data\n",
    "            batch_x,batch_y,epoch_temp,word_pointer,desc_pointer = generate_batch(batch_size,num_skips,\n",
    "                                                                                  skip_window,train_set_id,\n",
    "                                                                                  word_pointer,desc_pointer,\n",
    "                                                                                  epoch)\n",
    "            \n",
    "            _,loss = sess.run([train_opt,loss_func],feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "            \n",
    "            average_loss += loss\n",
    "            \n",
    "            if epoch_temp != epoch:\n",
    "                epoch = epoch_temp\n",
    "                if step_counter > 0:\n",
    "                    average_loss /= step_counter\n",
    "                    av_losses_list.append(average_loss)\n",
    "                    average_loss = 0\n",
    "                    step_counter = 0\n",
    "        \n",
    "        #normalize embeddings before using them\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims = True))\n",
    "        normalized_embedding = embedding/norm\n",
    "        normalized_embedding_matrix = sess.run(normalized_embedding)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # evaluate the model.\n",
    "    mean_loss = model_evaluation(testing_grams,normalized_embedding_matrix)\n",
    "    \n",
    "    save_log(av_losses_list,mean_loss,total_time,embedding_dim,skip_window,num_sampled,\n",
    "             learning_rate,total_epochs)\n",
    "    \n",
    "    return normalized_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "olympic-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to measure the quality of word's embeddings we compute the cosine similarity for pairs of words\n",
    "# that are true neigbhors. We define as loss the absolute value of 1-cosine similarity. \n",
    "# The total loss is the mean of them.\n",
    "# We want to minimize that loss, which means that words that are trully neighobrs we will be close in the \n",
    "# embedding space\n",
    "\n",
    "def model_evaluation(random_grams,embedding_matrix):\n",
    "    \n",
    "    total_loss    = 0 \n",
    "    total_counter = 0\n",
    "    \n",
    "    for exp in random_grams:\n",
    "        target_word = exp[0]\n",
    "        w_t         = embedding_matrix[target_word]\n",
    "        \n",
    "        for j in range(1,len(exp)):\n",
    "            context_word   = exp[j]\n",
    "            w_temp         = embedding_matrix[context_word]\n",
    "            result         = np.dot(w_t,w_temp)/(np.sqrt(np.dot(w_t,w_t))*np.sqrt(np.dot(w_temp,w_temp)))\n",
    "            total_loss    += abs(1-result) \n",
    "            total_counter += 1\n",
    "    \n",
    "    return total_loss/total_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "perfect-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(av_losses_list,mean_loss,total_time,embedding_dim,skip_window,\n",
    "             num_sampled,learning_rate,total_epochs):\n",
    "    \n",
    "    file = open(\"logs.txt\",\"a+\")\n",
    "    \n",
    "    file.write(\"EMBEDDING MODEL: embedding_dim = %s, skip_window = %s , total epochs = %s, learning rate = %s, negative samples = %s \\n\"\n",
    "              % (str(embedding_dim),str(skip_window),str(total_epochs),str(learning_rate),str(num_sampled)))\n",
    "    \n",
    "    file.write(\"average losses per epoch \\n\")\n",
    "    \n",
    "    for counter,value in enumerate(av_losses_list):\n",
    "        file.write(\"epoch %s , average lost %s \\n\"%(str(counter + 1),str(value)))\n",
    "    \n",
    "    file.write(\"training time in seconds %s \"%(str(total_time)))\n",
    "    file.write(\"evaluation result: %s \\n\"%(str(mean_loss)))\n",
    "    \n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_dim in embedding_dim_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "        for skip_window in skip_window_list:\n",
    "            for num_sampled in num_sampled_list:\n",
    "                for total_epochs in num_epochs_list:\n",
    "                    \n",
    "                    print(\"RUN EMBEDDINGS FOR: embedding dimension:\",embedding_dim,\"learning rate:\",\n",
    "                          learning_rate,\"skip window:\",skip_window,\"num_sampled:\",num_sampled,\n",
    "                          \"epochs:\",total_epochs)\n",
    "                    \n",
    "                    embedding_matrix = model_def(train_set_id,batch_size,embedding_dim,skip_window,\n",
    "                                                    num_skips,num_sampled,learning_rate,vocabulary_size,\n",
    "                                                    total_epochs,valid_grams)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "vanilla-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "skip_window   = 4\n",
    "num_sampled   = 64\n",
    "learning_rate = 0.1\n",
    "total_epochs  = 30\n",
    "embedding_matrix = model_def(train_set_id,batch_size,embedding_dim,skip_window,\n",
    "                                                    num_skips,num_sampled,learning_rate,vocabulary_size,\n",
    "                                                    total_epochs,valid_grams)\n",
    "\n",
    "np.savetxt('word_embeddings.txt', embedding_matrix, fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_word_embeddings(embedding_matrix):\n",
    "    \n",
    "    file = open(\"word_embeddings.txt\",\"w\")\n",
    "    for w_e in embedding_matrix:\n",
    "        for j in w_e:\n",
    "            file.write(\"%s,\"%(str(j)))\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
