{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "protecting-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word2vec model using the ski-gram approach and negative sampling. \n",
    "# For the implementation the library tensorflow was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "former-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kostas/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "turkish-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first time the below command should run to download stopwords\n",
    "#nltk.download('stopwords')\n",
    "all_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#initialize necessary variables \n",
    "dir_path      = '/home/kostas/Documents/thesis/data'\n",
    "descriptions  = []\n",
    "delimiter     = \"!n_s!\"\n",
    "unk_word      = \"UNK\"\n",
    "embedding_dim = 100\n",
    "min_occurance = 2\n",
    "num_sampled   = 64\n",
    "learning_rate = 0.1\n",
    "num_steps     = 5\n",
    "batch_size    = 100\n",
    "num_skips     = 2\n",
    "skip_window   = 3\n",
    "data_pointer  = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "racial-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(description,all_stopwords):\n",
    "    \n",
    "    #define translator to translate punctuation to white space\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #join all lines into one sentence\n",
    "    sentence     = ' '.join(description)\n",
    "    \n",
    "    #translate punctuation\n",
    "    new_sentence = sentence.translate(translator)\n",
    "    \n",
    "    #split the sentense in words\n",
    "    words = new_sentence.split()\n",
    "    words_sw = [w.lower() for w in words if not w.lower() in all_stopwords and len(w)>1]\n",
    "    \n",
    "    return words_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adapted-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read documents\n",
    "# clear the data and \n",
    "# tokenize it\n",
    "for fname in os.listdir(dir_path):\n",
    "    with open(os.path.join(dir_path,fname)) as json_file:\n",
    "        \n",
    "        #load data in json format\n",
    "        data = json.load(json_file)\n",
    "        for p in data:\n",
    "            issue_desc = p['description']\n",
    "            clean_desc = clean_data(issue_desc,all_stopwords)\n",
    "            descriptions.append(delimiter)\n",
    "            descriptions = descriptions + clean_desc\n",
    "            #print(\"##################\")\n",
    "            #print(issue_desc)\n",
    "            #print(\"\\n\")\n",
    "            #print(clean_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "geographic-collapse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create vocabulary \n",
    "#remove rare words from vocabulary which occurrs less time than min_occurance\n",
    "\n",
    "#descriptions:list which contains the sentences in tokens\n",
    "#word2id     :dictionary which contains the vocabulary and it's int id\n",
    "#id2word     :dictionary which assigns its id to corresponding word\n",
    "\n",
    "count  = []\n",
    "count.extend(collections.Counter(descriptions).most_common())\n",
    "for i in range(len(count)-1,-1,-1):\n",
    "    if count[i][1]<min_occurance:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#compute the vocabulary size substruct 1 cause we dont have to count the delimiter !n_s!\n",
    "vocabulary_size = len(count)-1\n",
    "    \n",
    "#assign an id to each word\n",
    "word2id = dict()\n",
    "word2id[unk_word] = -2\n",
    "for i, (word,_) in enumerate(count):\n",
    "    if word == delimiter:\n",
    "        word2id[word] =  -1\n",
    "    else:\n",
    "        word2id[word] = i\n",
    "\n",
    "#create and the opposite dictionary for easy search based on id\n",
    "id2word = dict(zip(word2id.values(),word2id.keys()))\n",
    "\n",
    "#express descriptions using id\n",
    "data = list()\n",
    "while len(descriptions) != 0:\n",
    "    temp    = descriptions.pop(0)\n",
    "    temp_id = word2id.get(temp,-2)\n",
    "    data.append(temp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equal-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skip_grams(batch_size,num_skips,skip_window,data):\n",
    "    \n",
    "    global data_pointer\n",
    "    \n",
    "    #some important constraints\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= skip_window\n",
    "    \n",
    "    # the batch stores target words\n",
    "    batch  = np.ndarray(shape = (batch_size),dtype = np.int32)\n",
    "    # labels are the context words=>(skip-grams)\n",
    "    labels = np.ndarray(shape = (batch_size,1), dtype = np.int32)\n",
    "    \n",
    "    #ski-gram=>you can see skip items right and left from current position\n",
    "    span = 2*skip_window+1\n",
    "    \n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        \n",
    "        #avoid tags with id -1,-2\n",
    "        while data[data_pointer] < 0:\n",
    "            data_pointer += 1\n",
    "        \n",
    "        #fill the buffer\n",
    "        find_context_words(buffer,data,skip_window,span)\n",
    "        \n",
    "        #take num_skips random samples \n",
    "        context_words = [w for w in range(1,len(buffer))]\n",
    "        words_to_use  = random.sample(context_words,num_skips)\n",
    "        \n",
    "        #print(\"target word\",data_pointer,\"words_to_use:\",words_to_use,\"buffer:\",buffer)\n",
    "        #update batch and labels\n",
    "        for j,random_word in enumerate(words_to_use):\n",
    "            batch[i*num_skips+j]    = buffer[0]\n",
    "            labels[i*num_skips+j,0] = buffer[random_word]\n",
    "            \n",
    "        #clear buffer for the next target word\n",
    "        buffer.clear()\n",
    "        \n",
    "        if data_pointer == len(data):\n",
    "            data_pointer = 0\n",
    "        else:\n",
    "            data_pointer += 1\n",
    "    \n",
    "    return batch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "continuous-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_context_words(buffer,data,skip_window,span):\n",
    "    \n",
    "    global data_pointer\n",
    "    counter   = 1\n",
    "    l_pointer = 1\n",
    "    r_pointer = 1\n",
    "    \n",
    "    #the target word in the first place\n",
    "    buffer.append(data[data_pointer])\n",
    "    \n",
    "    while counter<span: \n",
    "        #look left from target word\n",
    "        if counter <=skip_window:\n",
    "            #if encounter -1 => new description meaning that cannot take another word \n",
    "            if data[data_pointer - l_pointer] == -1:\n",
    "                counter = skip_window + 1\n",
    "            elif data[data_pointer - l_pointer] == -2:\n",
    "                l_pointer += 1\n",
    "            else:\n",
    "                buffer.append(data[data_pointer - l_pointer])\n",
    "                l_pointer += 1\n",
    "                counter   += 1\n",
    "        #look right from target word\n",
    "        else:\n",
    "            #if encounter -1 => new description meaning that cannot take another word\n",
    "            if data[data_pointer + r_pointer] == -1:\n",
    "                counter = span\n",
    "            #if encounter -2 => unknown word so we look the next one\n",
    "            elif data[data_pointer + r_pointer] == -2: \n",
    "                r_pointer += 1\n",
    "            else:\n",
    "                buffer.append(data[data_pointer + r_pointer])\n",
    "                r_pointer += 1\n",
    "                counter   += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "attended-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "#input data\n",
    "X_train = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "#input label\n",
    "Y_train = tf.placeholder(tf.int32,shape=[None,1])\n",
    "\n",
    "#ensure that the following ops & var are assigned to CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #create the embedding variable wich contains the weights\n",
    "    embedding = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "    \n",
    "    #create the lookup table for each sample in X_train=>avoiding to use one_hot encoder\n",
    "    X_embed   = tf.nn.embedding_lookup(embedding,X_train) \n",
    "    \n",
    "    #create variables for the loss function\n",
    "    nce_weights = tf.Variable(tf.random_normal([vocabulary_size,embedding_dim]))\n",
    "    nce_biases  = tf.Variable(tf.random_normal([vocabulary_size]))\n",
    "\n",
    "loss_func = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,biases =nce_biases, labels = Y_train, \n",
    "                                          inputs = X_embed, num_sampled = num_sampled, \n",
    "                                          num_classes = vocabulary_size ))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_opt = optimizer.minimize(loss_func)\n",
    "\n",
    "#Define initializer for tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #actual initialize the variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in range(1,num_steps+1):\n",
    "        \n",
    "        #take new batch of data\n",
    "        batch_x,batch_y = create_skip_grams(batch_size,num_skips,skip_window,data)\n",
    "        \n",
    "        #train\n",
    "        _,loss = sess.run([train_opt,loss_func], feed_dict={X_train:batch_x, Y_train:batch_y})\n",
    "        average_loss += loss\n",
    "        \n",
    "    \n",
    "    #normalize embeddings before using them\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keep_dims = True))\n",
    "    normalized_embedding = embedding/norm\n",
    "    normalized_embedding_matrix = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "authorized-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00429874  0.01452199 -0.23464274 -0.06807403  0.14553358 -0.03239543\n",
      "  0.03707422  0.00805677 -0.01922133 -0.11350194  0.0800465  -0.05834521\n",
      " -0.20336981 -0.09596892 -0.08024165 -0.2054867   0.05005193  0.10228378\n",
      " -0.07640604 -0.07165489  0.07979203  0.06147172 -0.00795392 -0.01453118\n",
      "  0.07355349  0.10884865  0.07255135 -0.03741981 -0.07674964 -0.23107524\n",
      " -0.07061283 -0.05199072 -0.16668801  0.11865239  0.0188407   0.08940855\n",
      "  0.10068807  0.02990313 -0.12422778  0.01374637  0.070717    0.3156952\n",
      "  0.02327913  0.03987381 -0.17283702  0.04432922  0.02615609 -0.00494789\n",
      "  0.00555748 -0.05504418  0.12788895 -0.16435462 -0.03153923 -0.0540764\n",
      " -0.05479822  0.05237039  0.03919997 -0.13805555  0.00802774  0.06373612\n",
      " -0.05269733 -0.02619855 -0.01170155  0.07926175  0.07835087 -0.20803055\n",
      "  0.06597845  0.04542603 -0.13976602 -0.24700351 -0.04573974  0.02913193\n",
      "  0.0525511  -0.05415343 -0.03480895 -0.0160954   0.049901    0.08804394\n",
      "  0.02673271 -0.0305111  -0.12584852 -0.07824893  0.14299542 -0.14860658\n",
      " -0.12055459 -0.11998776 -0.08705977 -0.06182763 -0.07916021  0.07506771\n",
      " -0.19470528  0.00771763  0.00319058  0.14189902 -0.06489103 -0.00509715\n",
      " -0.07887515  0.04527558  0.10211125  0.07617595]\n"
     ]
    }
   ],
   "source": [
    "example = normalized_embedding_matrix[word2id[\"geo\"]]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-revolution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
